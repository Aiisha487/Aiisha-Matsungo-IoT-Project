% ============================================================================
% SINGLE FILE VERSION - No separate sections needed
% ============================================================================
\documentclass[11pt]{article}

% PACKAGES
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{graphicx}

% SETTINGS
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\pagestyle{fancy}
\fancyhf{}
\rhead{EECE 5155 -- PES v1}
\lhead{Spring 2026}
\cfoot{\thepage}

% TITLE
\title{%
    \textbf{EECE 5155: Wireless Sensor Networks and IoT Systems}\\[0.5em]
    \Large Project Engineering Specification (PES) -- Version 1\\[1em]
    \large \textit{IoT-Based Acoustic Monitoring System for Designated Quiet Zones in Public Facilities}
}

\author{%
    \textbf{Student Name:} Aiisha Matsungo\\
    \textbf{NUID:} 002530298\\
    \textbf{GitHub Repo:} @Aiisha487\\
    \textbf{Track:} Implementation\\
    \textbf{Date:} February 4, 2026\\
    \textbf{Version:} 1.0
}

\date{}

\begin{document}

\maketitle
\thispagestyle{fancy}

% ============================================================================
% SECTION 1: PROBLEM DEFINITION
% ============================================================================
\section{Problem Definition and Use Case}
\label{sec:problem}

\noindent\textbf{Problem Statement:}\\
Public facilities designate quiet zones for health activities (meditation, prayer, sensory regulation, focused work) requiring acoustic levels below 40--50~dBA, but lack verification systems to ensure zones maintain standards. This results in poor placement decisions (zones near elevators achieve 50--65~dBA baseline), no user wayfinding (people cannot locate or verify quiet zones before traveling), and no compliance data (facilities cannot justify \$5,000--15,000 soundproofing investments without objective measurements).

\vspace{0.5em}

\noindent\textbf{Target User/Stakeholder:}\\
\textbf{Primary:} Individuals seeking quiet for any purpose (students studying, people meditating/praying, persons with sensory sensitivities managing overwhelm, hospital visitors processing emotions, travelers needing rest).
\textbf{Secondary:} Facility managers (libraries, hospitals, airports, hotels) need compliance verification and placement optimization data.

\vspace{0.5em}

\noindent\textbf{Current Baseline:}\\
Facilities post ``Quiet Zone'' signs with no enforcement or verification. Staff patrol intermittently (reactive, cannot monitor all zones). Users must physically enter zones to discover if they are actually quiet, often finding noisy spaces (60--70~dBA from conversations, poor placement). There are no data on zone effectiveness, placement quality, or utilization patterns. Facilities cannot justify improvement investments without objective acoustic measurements.

\vspace{0.5em}

\noindent\textbf{Why WSN/IoT:}\\
The wireless sensor network is appropriate because:
(1) Multiple distributed zones require monitoring (wired impractical for retrofits across building floors)
(2) existing facility WiFi infrastructure eliminates need for dedicated network
(3) continuous 24/7 monitoring needed (manual staff checks infeasible)
(4) real-time status display enables user wayfinding (immediate decision-making)
(5) cloud aggregation allows facility-wide analytics (identify patterns across zones impossible with standalone devices).

\vspace{0.5em}

\noindent\textbf{Alternatives Considered:}
\begin{enumerate}[noitemsep]
    \item \textbf{Handheld sound level meters with manual logging:} Rejected because spot-checks miss temporal variations (zone quiet at 3~AM, loud at noon), labor-intensive (staff time cost exceeds automated system), no real-time user information (cannot enable wayfinding).
    
    \item \textbf{Standalone battery-powered noise loggers:} Rejected because no real-time display capability (data downloaded weekly), no network connectivity (cannot aggregate multi-zone status), expensive (\$200--500 per unit vs \$30 for networked sensor), no user-facing interface.
\end{enumerate}
% ============================================================================
% PHOTO INTEGRATION 
% ============================================================================


\clearpage

% ============================================================================
% SITE SURVEY WITH IMAGES
% ============================================================================
% Insert this AFTER Section 1 (Problem Definition) ends
% BEFORE Section 2 (Deployment Scenario) starts

\clearpage

\subsection{Site Survey Documentation}
\label{subsec:sitesurvey}

Field observations conducted January~28, 2026 at Northeastern University Snell Library and local elderly care facility documented existing quiet zone implementations and acoustic challenges. Visual documentation validates problem statement: facilities designate quiet zones via signage but lack verification systems to ensure acoustic standards are maintained.

\vspace{1em}

% ----------------------------------------------------------------------------
% FIGURE 1: Glass Room - Poorly Placed Zone
% ----------------------------------------------------------------------------
\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{snell_glass_room.jpg}
\caption{\textbf{Snell Library 4th Floor Glass Study Room -- Poorly-Placed Quiet Zone:} All-glass construction (floor-to-ceiling glass walls visible on multiple sides) allows visual transparency but provides minimal acoustic isolation (estimated STC 26--28 per typical tempered glass specifications). Room surrounded by open corridor with foot traffic and adjacent open workspaces (visible through glass). Despite posted ``quiet zone'' designation, predicted baseline: 42--48~dBA due to noise transmission through glass and proximity to high-traffic areas. Validates core problem: architectural decisions (glass for openness/visibility) conflict with acoustic requirements (<40~dBA standard). WiFi infrastructure visible (ceiling-mounted access points), standard power outlets accessible. This represents target deployment environment for sensor validation -- system should detect elevated baseline and flag for facility management review.}
\label{fig:glass_room}
\end{figure}

\vspace{0.5em}

% ----------------------------------------------------------------------------
% FIGURE 2: Signage Without Verification
% ----------------------------------------------------------------------------
\begin{figure}[htbp]
\centering
\includegraphics[width=0.55\textwidth]{silent_study_sign.jpg}
\caption{\textbf{Posted Signage Without Acoustic Verification:} ``Silent Study'' sign at Snell Library entrance requests behavioral compliance (``Please be respectful, No talking, Silence phones and devices, Thank you for your cooperation'') but provides no objective acoustic threshold (e.g., ``<45~dBA required'') or real-time feedback on current sound levels. Users cannot determine if zone is \textit{actually quiet} before entering -- must physically enter room to assess conditions. Validates problem statement (Section~\ref{sec:problem}): facilities rely on signage alone with no enforcement, monitoring, or verification mechanism. This gap creates user frustration (traveled to zone only to find it noisy) and prevents facility data-driven improvements (no objective measurements to justify soundproofing investments). Photo taken at blue-walled individual study room with glass door visible in background.}
\label{fig:sign}
\end{figure}

\clearpage

% ----------------------------------------------------------------------------
% FIGURE 3: Healthcare Reflection Room
% ----------------------------------------------------------------------------
\begin{figure}[htbp]
\centering
\includegraphics[width=0.55\textwidth]{reflection_room.jpeg}
\caption{\textbf{Healthcare Facility Reflection Room -- Moderate Placement Quality:} Solid drywall construction with French-style glass door (9--12 lite pattern visible) at elderly care center near Northeastern campus. Door signage: ``Reflection Room -- A place for peace'' (no numeric acoustic standard posted). Located adjacent to elevator bank and waiting area visible in background -- potential noise sources include conversations (50--60~dBA), elevator mechanisms (45--55~dBA), foot traffic. Represents \textit{moderate} placement quality: better acoustic isolation than glass rooms (estimated STC 45--50 for drywall assembly vs STC 26--28 glass) but compromised by proximity to high-traffic areas. Predicted baseline: 38--42~dBA when unoccupied. Solid wall construction provides 10--15~dBA improvement over glass rooms, validating sensor placement strategy for MVP (Section~\ref{sec:mvp}): deploy in contrasting construction types to demonstrate placement quality detection capability.}
\label{fig:reflection}
\end{figure}

\vspace{0.5em}

% ----------------------------------------------------------------------------
% FIGURE 4: Additional Silent Study Zone
% ----------------------------------------------------------------------------
\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{silent_study_yellow}
\caption{\textbf{Snell Library Silent Study Zone -- Semi-Enclosed Design:} Study room featuring decorative accent wall (yellow with blue circular network pattern graphic) and glass entrance door visible on left. Identical ``Silent Study'' signage on easel stand demonstrates consistent signage policy across facility without corresponding acoustic verification infrastructure. More enclosed than 4th floor all-glass room (Fig.~\ref{fig:glass_room}) but still incorporates glass door/wall elements for visibility. Furniture visible: upholstered chairs (coral/orange accent chair foreground, gray chairs at small table), demonstrating designated quiet study space. Acoustic treatment: standard carpet flooring (blue-gray pattern visible), painted drywall walls, no specialized sound absorption panels or acoustic ceiling tiles observed. Estimated baseline: 40--45~dBA (intermediate between all-glass 42--48~dBA and solid-wall 38--42~dBA configurations). Ceiling infrastructure: suspended linear LED fixtures, HVAC diffusers, building systems typical of modern institutional construction.}
\label{fig:yellow_room}
\end{figure}

\vspace{1em}

% ----------------------------------------------------------------------------
% SITE SURVEY FINDINGS SUMMARY
% ----------------------------------------------------------------------------
\noindent\textbf{Site Survey Findings Summary:}

\vspace{0.5em}

\noindent\textit{Construction Type Variability:} Glass-walled rooms (Figs.~\ref{fig:glass_room}, \ref{fig:yellow_room}) versus solid-wall room (Fig.~\ref{fig:reflection}) demonstrate predicted 10--15~dBA baseline difference based on Sound Transmission Class (STC) ratings: tempered glass assemblies STC 26--28 allow significant noise intrusion, while standard drywall partitions STC 45--50 provide superior acoustic isolation. This variability validates core system requirement: placement quality detection capability enabling facility managers to identify underperforming zones requiring architectural remediation (e.g., adding acoustic panels, relocating designation to better-isolated room).

\vspace{0.5em}

\noindent\textit{Signage Policy Without Enforcement:} All surveyed facilities (3 locations, 4 designated zones) post ``Silent Study'' or ``Quiet Zone'' signage (Fig.~\ref{fig:sign}) requesting behavioral compliance but provide: (1) no numeric acoustic threshold specifications (e.g., ``<45~dBA maintained''), (2) no real-time status indicators enabling user wayfinding (``Currently: 38~dBA -- Compliant''), (3) no monitoring equipment (zero sound level meters observed), (4) no historical compliance data informing facility investment decisions. Gap directly validates problem statement (Section~\ref{sec:problem}): verification system needed.

\vspace{0.5em}

\noindent\textit{Deployment Infrastructure Verified:} All target locations confirmed feasible for sensor deployment: (1) Standard 120V AC power outlets observed at floor/wall level in all rooms (eliminates battery constraints), (2) WiFi infrastructure operational -- eduroam and NUwave SSIDs detected via smartphone WiFi analyzer with RSSI -45 to -55~dBm (excellent signal strength, no extenders required), (3) wall-mounting space available at 1.5m height (typical signage mounting height) for sensor placement without obstructing room function, (4) HVAC environmental control present (temperature 20--22°C measured, within MAX9814 -40 to +85°C operating range with large margin). No environmental barriers to deployment identified.

\vspace{0.5em}

\noindent\textit{Placement Challenges Documented:} High-traffic adjacencies common across institutional facilities due to competing space-use priorities: (1) Elevators/waiting areas generate 45--60~dBA baseline from mechanical systems + user conversations (Fig.~\ref{fig:reflection}), (2) open corridors with foot traffic contribute 50--65~dBA transient noise (Figs.~\ref{fig:glass_room}, \ref{fig:yellow_room}), (3) glass construction prioritized for supervision/visibility over acoustic isolation (common in libraries for staff monitoring). Facilities face inherent trade-off: accessibility and openness versus acoustic performance. Continuous monitoring enables data-driven decisions: accept elevated baseline in glass room (reclassify as ``Quiet'' 50~dBA vs ``Silent'' 40~dBA), or invest in acoustic treatment (\$500--2000 panels), or relocate designation to interior room with better isolation.

\vspace{0.5em}

\noindent\textbf{Impact on System Design:} Site survey directly informed: (1) \textit{Deployment scenario} (Section~\ref{sec:deployment}): sensor placement strategy prioritizes contrasting construction types (glass 42--48~dBA vs solid 38--42~dBA) to validate placement quality detection -- predicted 10--15~dBA difference serves as MVP Success Criterion~\#8, (2) \textit{Threshold selection}: 45~dBA threshold chosen as midpoint between well-placed solid-wall zones (38--42~dBA achievable) and poorly-placed glass zones (42--48~dBA baseline), enabling binary classification while accommodating realistic institutional constraints, (3) \textit{Sustained violation logic}: 5-minute duration selected to filter transient corridor noise (<2~min door slams, brief conversations) while detecting sustained occupant talking (>5~min conversations at 60--70~dBA) that violates quiet zone purpose.

\clearpage

\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\textwidth]{reflection_room.jpeg}
\caption{\textbf{Healthcare Facility Reflection Room:} Solid drywall construction with French doors at elderly care center near Northeastern campus. Located adjacent to elevator and waiting area (potential noise sources: 50--60~dBA from conversations, elevator mechanisms 45--55~dBA). Represents \textit{moderate} placement quality -- better acoustic isolation than glass rooms but compromised by proximity to high-traffic areas. Door signage: ``Reflection Room -- A place for peace.'' No numeric acoustic standard posted. Predicted baseline: 38--42~dBA when unoccupied.}
\label{fig:reflection}
\end{figure}

\vspace{1em}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{silent_study_yellow.jpg}
\caption{\textbf{Snell Library Additional Silent Study Zone:} Room with decorative wall (yellow with blue network pattern) and posted ``Silent Study'' sign on glass entrance. More enclosed than 4th floor glass room but still features glass door/walls. Sign identical to Fig.~\ref{fig:sign}, demonstrating consistent signage policy across facility without corresponding verification infrastructure. Furniture visible (chairs, small table) indicates designated study space. Acoustic treatment: standard carpet flooring, painted walls (no specialized sound absorption materials observed).}
\label{fig:yellow_room}
\end{figure}

\vspace{1em}

\noindent\textbf{Site Survey Observations Summary:}
\begin{itemize}[noitemsep]
    \item \textbf{Construction variability:} Glass-walled rooms (Figs.~\ref{fig:glass_room}, \ref{fig:yellow_room}) vs solid-wall rooms (Fig.~\ref{fig:reflection}) demonstrate 10--15~dBA baseline difference predicted by STC ratings (STC 26--28 glass vs STC 45--50 drywall).
    \item \textbf{Signage consistency:} All facilities post ``Silent'' or ``Quiet'' designations but specify no numeric acoustic threshold (e.g., ``<45~dBA required''). Users have no objective measure of compliance.
    \item \textbf{No verification systems observed:} Zero sound level meters, monitoring equipment, or real-time status displays detected during 2-hour site survey across 3 locations. Validates gap identified in problem statement.
    \item \textbf{Placement challenges:} High-traffic adjacencies common (elevators, corridors, open workspaces) due to space constraints in institutional buildings. Facilities prioritize accessibility over acoustic isolation.
    \item \textbf{Deployment feasibility confirmed:} Standard 120V outlets observed in all rooms, WiFi coverage verified via smartphone (eduroam RSSI: -45 to -55~dBm, excellent). Wall-mounting space available at 1.5m height. No environmental barriers to sensor installation identified.
\end{itemize}

\noindent\textbf{Photographic Evidence Impact on Design:} Site survey directly informed deployment scenario (Section~\ref{sec:deployment}) sensor placement strategy: prioritize contrasting construction types (glass vs solid walls) to validate system's placement quality detection capability. Predicted 10--15~dBA baseline difference between Fig.~\ref{fig:glass_room} (glass, 42--48~dBA) and Fig.~\ref{fig:reflection} (solid walls, 38--42~dBA) serves as key validation criterion (Success Criterion~\#8, Section~\ref{sec:mvp}).

\clearpage


% ============================================================================
% SECTION 2: DEPLOYMENT
% ============================================================================
\section{Deployment Scenario and Scale}
\label{sec:deployment}

\noindent\textbf{Environment Description:}\\
Target deployment: Designated quiet zones within institutional buildings (libraries, hospitals, airports). Indoor environment with HVAC control (15--30°C, 30--70\% RH). Acoustic challenges include external noise intrusion (rail lines, traffic: 50--70~dBA), internal sources (elevators, HVAC: 45--60~dBA), and user behavior (conversations: 60--75~dBA). Building construction typically concrete/drywall with hard reflective surfaces or glass partitions. WiFi infrastructure present (802.11g/n, multiple access points). Standard 120V AC power outlets available for continuous sensor operation.

\vspace{0.5em}

\noindent\textbf{Topology and Placement:}\\
\textbf{Network topology:} Star topology with sensors connecting directly to existing facility WiFi access points, then to cloud via internet (no dedicated gateway for MVP). \textbf{Justification:} Existing WiFi infrastructure eliminates \$150+ gateway cost (33\% of budget saved); star topology simplest for proof-of-concept; sensors placed statically (no mobility requires mesh); 2--3 nodes, all within WiFi range (no multi-hop needed). \textbf{Node placement:} One sensor per designated quiet zone, 1.5 m height wall-mounted, central to room coverage area. For 2-sensor MVP: prioritize zones with (a) different acoustic characteristics (well-placed vs poorly-placed comparison validates problem statement), (b) varied construction types (glass vs solid walls demonstrates placement quality detection capability).

\vspace{1em}

Site survey photos (Figs.~\ref{fig:glass_room}--\ref{fig:yellow_room}) document 
target deployment environments and validate WiFi infrastructure availability.

\begin{table}[htbp]
\centering
\caption{Table A: Deployment Scale and Assumptions}
\label{tab:deployment}
\begin{tabular}{@{}p{5cm}p{3cm}p{5cm}@{}}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Justification/Source} \\
\midrule
Deployment Area & 800 m\textsuperscript{2} & 2--3 quiet zones × 250--300 m\textsuperscript{2} each (typical study room size) \\
\addlinespace
Number of Sensor Nodes & 2--3 & One sensor per quiet zone (MVP: 2; stretch: 3); scalable to 5--10 for full facility \\
\addlinespace
Number of Gateways & 0 & Direct WiFi to cloud (existing infrastructure); gateway optional for 10+ sensors \\
\addlinespace
Environment Type & Indoor & Institutional buildings (libraries, hospitals); HVAC controlled \\
\addlinespace
Node Mobility & Static & Fixed mounting per quiet zone; no relocation during operation \\
\addlinespace
Duty Cycle (\%) & 100\% & Continuous monitoring (wall-powered; no energy constraint); always-on WiFi \\
\addlinespace
Expected Deployment Lifetime & 2+ years & Electronic components MTBF >10,000 hrs; semi-annual recalibration extends accuracy \\
\addlinespace
Operating Temperature Range & 15 to 30°C & Indoor HVAC controlled; MAX9814 rated -40 to +85°C (70°C margin) \\
\bottomrule
\end{tabular}
\end{table}

\clearpage

% ============================================================================
% SECTION 3: SIGNALS AND DATA MODEL
% ============================================================================
\section{Signals and Data Model}
\label{sec:signals}

\noindent\textbf{Signal List:}
\begin{enumerate}[noitemsep]
    \item \textbf{Sound Pressure Level (dBA SPL):} Acoustic pressure magnitude, units: decibels referenced to 20~µPa, sensor: MAX9814 electret microphone with amplifier, resolution: $\pm$3~dBA (calibrated system accuracy), range: 35--100~dBA SPL, sampling rate: 1~Hz effective output (1~kHz raw ADC with 1-second RMS averaging + 10-sample moving average filter), justification: 1~Hz adequate for monitoring quasi-stationary quiet zone conditions (speech events $>$10~sec duration); higher rates unnecessary (not analyzing speech content or performing frequency-domain analysis); sub-Nyquist sampling acceptable for amplitude-only measurement (RMS energy preserved under aliasing per Parseval's theorem).
    
    \item \textbf{Timestamp (Unix epoch):} Measurement time via NTP-synchronized ESP8266 real-time clock, units: seconds since 1970-01-01 00:00:00 UTC, resolution: 1~second, sampling rate: Per dBA reading (every 30~seconds with transmitted packet), justification: Multi-sensor event correlation requires synchronized timestamps; 1-second precision sufficient for minute-scale quiet zone status updates and historical trend analysis.
    
    \item \textbf{Zone ID (string):} Sensor location identifier, format: ``snell\_floor2\_silent'' (alphanumeric), length: 20~characters maximum, sampling rate: Static metadata per packet (does not change), justification: Enables multi-zone dashboard display with per-location filtering and facility-wide analytics (comparison across zones, identification of consistently non-compliant areas).
\end{enumerate}

\vspace{0.5em}

\noindent\textbf{Packet Structure Concept:}\\
JSON format transmitted via HTTP POST every 30~seconds:
\begin{verbatim}
{
  "zone_id": "snell_floor2_silent",    // 20 bytes
  "timestamp": 1706745600,              // 10 bytes (Unix epoch)
  "dBA_spl": 42.3,                       // 4 bytes (float, 1 decimal)
  "status": "compliant"                 // 9 bytes (enum: compliant/violation)
}
\end{verbatim}

Payload breakdown: 94~bytes JSON content (uncompressed, no whitespace). HTTP request adds 169~bytes headers (POST method + Host + Content-Type + Content-Length + Connection). Complete transmission includes protocol overhead: TCP handshake/teardown (482~bytes), IPv4 headers (140~bytes for 7~packets), WiFi 802.11 framing (266~bytes for 7~frames). \textbf{Total: 1,063~bytes} transmitted per 30-second report. Overhead ratio: 969 / 94 = 10.3$\times$ (typical for HTTP over TCP/IP with short-lived connections). Alternative protocols (MQTT, binary encoding) could reduce overhead but provide minimal benefit for 30-second reporting interval (bandwidth unconstrained at 0.012\% WiFi utilization).

\vspace{0.5em}

\noindent\textbf{Derived Values:}\\
\textbf{Compliance status:} Binary classification derived from dBA SPL threshold comparison (``compliant'' if $<$45~dBA, ``violation'' if $\geq$45~dBA sustained $>$5~minutes). \textbf{Sustained violation logic:} 5-minute timer tracks consecutive 1-second samples above threshold; resets if returns to compliant state; prevents false alarms from transient noise (door slams $<$30~sec, brief conversations $<$2~min). \textbf{Moving average filter:} 10-sample window (10-second smoothing) applied to 1~Hz RMS stream before threshold comparison; attenuates fluctuations faster than 10~seconds (transients) while passing sustained events (conversations typically $>$30~sec duration). \textbf{Baseline drift (optional):} Diagnostic field computed as delta from calibrated baseline (stored in EEPROM); flags sensor aging or environmental changes requiring recalibration if drift $>$2~dBA sustained over 24~hours.

\clearpage

% ============================================================================
% SECTION 4: SYSTEM CONSTRAINTS ANALYSIS
% ============================================================================
\section{System Constraints Analysis}
\label{sec:constraints}

\noindent\textbf{Prioritized Constraints:}

\begin{enumerate}
    \item \textbf{Cost -- Highest Priority}\\
    Target: Total system $<$\$200 for 2--3 sensor MVP\\
    Design Impact: Drives component selection (ESP8266 \$6 vs Raspberry Pi \$35 saves \$87 for 3~nodes); mandates WiFi reuse vs LoRa gateway (\$150 savings); constrains to proof-of-concept scale (2--3 sensors, not 10+ commercial deployment); favors analog microphone (\$10 MAX9814) despite I2S digital alternative being \$3 cheaper (implementation simplicity valued over marginal cost savings given 8-hour debug risk premium); limits calibration equipment to consumer-grade meter (\$30 BAFX3608 vs \$300 precision instrument, acceptable for $\pm$3~dBA target).
    
    \item \textbf{Measurement Accuracy}\\
    Target: $\pm$3~dBA SPL across 35--100~dBA range\\
    Design Impact: Requires 6-point calibration procedure with reference meter (linear curve fit vs consumer-grade BAFX3608 $\pm$1.5~dBA); dictates minimum sensor SNR (61~dBA MAX9814 provides 12~dBA margin above 45~dBA threshold, adequate); necessitates error budget analysis (microphone $\pm$1~dBA + ADC $\pm$0.5~dBA + calibration $\pm$1~dBA + temperature $\pm$0.5~dBA = $\pm$1.6~dBA RSS, meeting $\pm$3~dBA target with 1.9$\times$ margin); validates 10-bit ADC adequacy (0.06~dBA/step resolution $\ll$ $\pm$3~dBA system accuracy).
    
    \item \textbf{Implementation Feasibility (Timeline)}\\
    Target: Complete design, build, calibrate, test, document within 10-week semester\\
    Design Impact: Favors simple analog microphone (analogRead() function, 2-hour implementation) over complex I2S digital interface (clock configuration, DMA setup, 8+ hour debug risk with uncertain outcome); prioritizes existing campus WiFi infrastructure (zero setup time) over custom LoRa/Zigbee network (16+ hours gateway configuration); requires modular design (each sensor independently functional, easy to add/remove without system reconfiguration); drives phased testing approach (lab validation with controlled environment first, facility deployment as stretch goal if permissions obtained by Week~6).
    
    \item \textbf{User Experience Latency}\\
    Target: Dashboard display updates $<$120~seconds after acoustic event\\
    Design Impact: Sets 30-second reporting interval (balances $<$120~sec requirement vs ThingSpeak API free tier 3M messages/year limit); enables WiFi always-on operation (no sleep/wake cycles adding latency); influences cloud polling strategy (30-second client refresh acceptable); not critical for historical analytics or facility management decisions (those use daily/weekly trends) but essential for real-time user wayfinding (``Which zone is quiet RIGHT NOW?'' decision window 1--5~minutes).
    
    \item \textbf{Power Availability}\\
    Target: Continuous operation from standard USB wall power (5V, 500~mA available capacity)\\
    Design Impact: Eliminates battery constraints entirely (enables 100\% duty cycle with no energy budgeting); allows WiFi always-on (simplifies firmware, no connection establishment delays); removes energy harvesting consideration (solar insufficient for indoor 445~mW requirement); permits higher-power MCU selection (ESP8266 81~mA average acceptable vs requiring ultra-low-power STM32L072 3~mA if battery-powered); annual electricity cost negligible (\$0.59 per sensor at \$0.15/kWh). Power is explicitly NOT a constraint for this wall-powered application; ranked \#5 to acknowledge consideration but demonstrate it's unconstrained.
\end{enumerate}

\vspace{0.5em}

\noindent\textbf{Constraint Conflicts and Tradeoffs:}\\
\textbf{Conflict 1 - Cost vs Accuracy:} ESP8266 10-bit ADC (lower cost \$6) provides only 0.06~dBA resolution vs 12-bit alternatives (\$15+) offering 0.015~dBA resolution. \textbf{Resolution:} 10-bit adequate; 0.06~dBA $\ll$ $\pm$3~dBA system target. Careful 6-point calibration compensates for ADC limitations. \textbf{Trade-off accepted:} Slightly coarser ADC resolution (still 50$\times$ finer than accuracy requirement) enables \$27 savings for 3~nodes, which can be allocated to validation equipment or contingency. \textbf{Conflict 2 - Timeline vs Accuracy:} I2S digital microphone (SPH0645, \$7) offers 65~dBA SNR ($\pm$2~dBA achievable accuracy) but requires 8+ hours I2S protocol implementation with medium-high debug risk. Analog microphone (MAX9814, \$10) provides 61~dBA SNR ($\pm$3--5~dBA accuracy) with 2-hour analogRead() implementation and low risk. \textbf{Resolution:} Both meet $\pm$3~dBA target after calibration; select analog to reduce schedule risk. \textbf{Trade-off accepted:} 4~dBA worse SNR (functionally irrelevant: both provide $>$9~dBA margin above detection threshold) + \$3 higher cost in exchange for 6-hour time savings and lower project failure risk. For 10-week student timeline, schedule certainty valued over marginal spec improvement.

\clearpage

% ============================================================================
% SECTION 5: TRAFFIC AND CAPACITY ESTIMATION
% ============================================================================
\section{Traffic and Capacity Estimation}
\label{sec:traffic}

\noindent\textbf{Traffic Calculations:}
\begin{verbatim}
Application layer (JSON payload):
  94 bytes data: {"zone_id": "snell_floor2_silent",
                  "timestamp": 1706745600,
                  "dBA_spl": 42.3,
                  "status": "compliant"}

HTTP layer:
  169 bytes headers: POST /update?api_key=XXX HTTP/1.1
                     Host: api.thingspeak.com
                     Content-Type: application/json
                     Content-Length: 94
                     Connection: close
  Total HTTP: 263 bytes (169 headers + 94 payload)

Transport layer (TCP):
  Connection establishment (3-way handshake): 270 bytes (3 packets)
  Data segment header: 32 bytes
  Connection teardown (FIN-ACK): 180 bytes (2 packets)
  TCP total: 482 bytes

Network layer (IPv4):
  7 packets × 20 bytes/header = 140 bytes

Data link layer (WiFi 802.11):
  7 frames × 38 bytes/frame = 266 bytes (header + FCS)

Complete transmission: 94 + 169 + 482 + 140 + 266 = 1,151 bytes
Protocol overhead: 1,151 - 94 = 1,057 bytes (11.2× overhead ratio)

Per-node data rate:
  1,151 bytes / 30 seconds = 38.4 bytes/s = 307 bps

Number of nodes: 3 (MVP scope: 2-3 sensors)

Aggregate data rate: 3 × 307 bps = 921 bps

Peak traffic factor: 1.5 (occasional simultaneous transmissions;
                          WiFi CSMA/CA reduces collision probability)
Peak data rate: 921 × 1.5 = 1,382 bps

Daily data volume: 38.4 bytes/s × 86,400 s/day × 3 nodes
                 = 9.96 MB/day
Monthly: 9.96 MB/day × 30 days = 299 MB/month
\end{verbatim}

\vspace{1em}

\begin{table}[htbp]
\centering
\caption{Table B: Traffic Budget}
\label{tab:traffic}
\begin{tabular}{@{}p{4.5cm}p{3cm}p{5.5cm}@{}}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Calculation/Notes} \\
\midrule
Payload Size (bytes) & 1,151 & 94 JSON + 169 HTTP + 482 TCP + 140 IP + 266 WiFi \\
\addlinespace
Sampling Interval (s) & 1 & 1~Hz dBA SPL output from 1~kHz raw ADC (RMS averaging) \\
\addlinespace
Reporting Interval (s) & 30 & Balance latency ($<$120~s req.) vs API limits (3M msg/year) \\
\addlinespace
Per-Node Data Rate (bps) & 307 & 1,151 bytes / 30~s $\times$ 8 bits/byte \\
\addlinespace
Number of Nodes & 3 & 2--3 quiet zones (Table~\ref{tab:deployment}); using 3 for worst-case \\
\addlinespace
Aggregate Data Rate (bps) & 921 & 3 nodes $\times$ 307~bps/node \\
\addlinespace
Protocol Overhead Factor & 11.2$\times$ & Already in payload (94 data $\rightarrow$ 1,151 total); HTTP/TCP/IP/WiFi \\
\addlinespace
Peak Traffic Factor & 1.5 & Occasional simultaneous TX; CSMA/CA collision avoidance \\
\addlinespace
Peak Data Rate (bps) & 1,382 & 921~bps $\times$ 1.5 peak factor \\
\addlinespace
Daily Data Volume (MB) & 9.96 & 38.4~bytes/s $\times$ 86,400~s/day $\times$ 3~nodes $\div$ 1,048,576 \\
\bottomrule
\end{tabular}
\end{table}

\vspace{0.5em}

\noindent\textbf{Capacity Comparison:}\\
WiFi 802.11g minimum capacity: 11~Mbps = 11,000,000~bps (IEEE 802.11-2007 standard). System peak rate: 1,382~bps. Utilization: 1,382 / 11,000,000 = \textbf{0.013\%} (99.987\% headroom). Network bandwidth NOT a constraint; could support 8,000+ sensors on single WiFi access point before congestion. ThingSpeak cloud API limit: 3,000,000~messages/year per free-tier channel. System generates: 3~sensors $\times$ (86,400~s/day $\div$ 30~s) $\times$ 365~days = 3,153,600~messages/year (105\% of single-channel limit). \textbf{Mitigation:} Use 3~separate ThingSpeak channels (1~per sensor) = 9,000,000~message/year total capacity, well within free tier. Cloud API becomes bottleneck before network bandwidth; bandwidth is trivially available.

\clearpage

% ============================================================================
% SECTION 6: OPERATIONAL MODEL AND CONCEPTUAL BOM
% ============================================================================
\section{Operational Model and Conceptual BOM}
\label{sec:bom}

\noindent\textbf{Operational Workflow:}\\
\textbf{Sense:} MAX9814 microphone transduces acoustic pressure (Pa) to analog voltage (0--3.3V, biased at 1.23V DC). Electret capsule powered by internal MICBIAS (2.0V). \textbf{Sample:} ESP8266 10-bit ADC samples MICOUT voltage at 1~kHz via analogRead() function. \textbf{Process:} Microcontroller computes 1-second RMS over 1000~samples, applies calibration curve (ADC value $\rightarrow$ dBA SPL via linear regression coefficients stored in EEPROM: dBA = m $\times$ ADC + b), computes 10-sample moving average (10-second smoothing window), detects sustained threshold violations ($>$45~dBA for $>$5~consecutive minutes triggers ``violation'' status). \textbf{Transmit:} Format JSON packet with zone\_id, timestamp (NTP-synced), dBA\_spl, status; HTTP POST via WiFi (802.11g) to ThingSpeak REST API every 30~seconds. \textbf{Store:} ThingSpeak time-series database retains 2+ weeks data (configurable retention). \textbf{Display:} Web dashboard (HTML/CSS/JavaScript) polls ThingSpeak API every 30~seconds, displays real-time status color-coded per zone (green $<$45~dBA compliant, red $\geq$45~dBA violation), shows last update timestamp and current dBA level. \textbf{Alert:} Email notification sent to facility staff if sustained violation detected ($>$5~minutes above threshold); user wayfinding enabled by real-time dashboard status (check before traveling to zone).

\vspace{0.5em}

\noindent\textbf{Power Strategy:}\\
Wall-powered via USB (5V, 1A wall adapters, standard phone chargers repurposed). Per-node consumption: 89.1~mA @ 5V = 445~mW continuous. Components: ESP8266 average 81~mA (140~mA transmit 0.5s/30s + 80~mA idle 29.5s/30s per datasheet Table~5-3), MAX9814 3.1~mA (datasheet Table~2), AMS1117-3.3 LDO regulator 5~mA quiescent + 143~mW dropout loss. Annual energy: 445~mW $\times$ 8,760~hr = 3.90~kWh/sensor (\$0.59/year @ \$0.15/kWh typical residential rate). Justification: Institutional facilities have accessible 120V AC outlets in study rooms and corridors; continuous operation required for 0\% data loss (no battery duty-cycling gaps); eliminates maintenance burden (no battery replacement every 6--12~months). No battery backup designed (accept power outage data gaps as non-critical for monitoring application; UPS available in critical facilities if needed). Power availability is unconstrained; enables design simplifications (always-on WiFi, no sleep mode programming, no energy harvesting complexity).

\vspace{0.5em}

\noindent\textbf{Maintenance Model:}\\
\textbf{Weekly (5~minutes):} Dashboard review via web browser, verify all sensors reporting (green heartbeat indicators), check for sustained violations requiring staff attention. \textbf{Monthly (30~minutes):} Spot-check calibration accuracy for 1~randomly-selected sensor using handheld BAFX3608 reference meter (measure same environment simultaneously, verify agreement within $\pm$3~dBA); inspect physical mounting (ensure sensors not tampered with or displaced); verify WiFi connectivity strength via ESP8266 serial console RSSI reporting. \textbf{Semi-annual (3~hours):} Full recalibration procedure for all sensors (6-point curve fit at 40, 50, 60, 70, 80, 94~dBA using tone generator + reference meter); update EEPROM calibration coefficients; validate at intermediate levels (45, 65, 85~dBA). \textbf{Annual (1--2~hours):} Hardware inspection for component aging (capacitor bulging, connector corrosion), firmware update if bugs discovered, consideration of component replacement (ESP8266 or MAX9814 modules \$16 each if performance degrades). \textbf{Total maintenance burden:} Approximately 15~hours/year for 3-sensor system (5~hours/year per sensor amortized). \textbf{Failure response:} Sensor offline detected via missing heartbeat (no data received $>$2~minutes triggers dashboard alert); troubleshooting via power cycle (USB disconnect/reconnect, 1~minute); hardware swap if persistent (\$31 replacement sensor node, 15~minutes installation).

\vspace{0.5em}

\noindent\textbf{Conceptual BOM:}
\begin{itemize}[noitemsep]
    \item \textbf{Sensor Nodes:} 3~units @ \$31 each = \$93 total. Per-node: ESP8266 NodeMCU v3 (\$6, Amazon/AliExpress), MAX9814 microphone breakout with integrated electret capsule (\$10, Adafruit \#1713), USB-A to Micro-B cable 6ft (\$3, AmazonBasics), 5V 1A USB wall adapter (\$2, generic/Anker), ABS plastic enclosure Hammond 1591 series (\$8, professional appearance for institutional setting), 3M Command strips damage-free mounting (\$2). Note: 2~sensors minimum for MVP (glass room + solid wall comparison), 3rd sensor stretch goal if time/budget permits.
    \item \textbf{Gateways:} 0~units @ \$0 (direct WiFi-to-cloud architecture eliminates gateway requirement; star topology with existing infrastructure)
    \item \textbf{Backend/Cloud:} \$0/month ongoing cost (ThingSpeak free tier: 3~channels $\times$ 3,000,000~messages/year = 9M total capacity vs 3.15M required; MATLAB visualization tools included; web API for dashboard integration). Alternative: AWS IoT Core (\$5/month estimated for this traffic) considered but rejected (free tier sufficient for proof-of-concept; paid services deferred to production scaling).
    \item \textbf{Validation Equipment:} \$30 BAFX3608 Digital Sound Level Meter (ANSI S1.4-2014 Class~2 certified, $\pm$1.5~dBA, Amazon B00ECCZWWI, required for calibration procedure) + \$15 multimeter for voltage verification (Harbor Freight, troubleshooting) + \$10 breadBAoard + jumper wire kit (electronics prototyping, Amazon) = \$55 total validation/development tools.
    \item \textbf{Miscellaneous:} Included in sensor node cost above (USB cables, power supplies, mounting hardware). Optional: \$6 spare ESP8266 + \$10 spare MAX9814 for failure redundancy (\$16 contingency components).
    \item \textbf{Total Estimated Cost:} \$148 core system (sensor nodes + validation equipment); \$164 with optional spare components; \textbf{budget remaining \$36--52} for unforeseen needs (additional sensors if easy deployment opportunity arises, replacement components if failures during testing, upgraded enclosures if aesthetic requirements stricter than anticipated).
\end{itemize}

\clearpage

% ============================================================================
% SECTION 7: RISKS, ASSUMPTIONS, AND UNKNOWNS
% ============================================================================
\section{Risks, Assumptions, and Unknowns}
\label{sec:risks}

\noindent\textbf{Technical Risks:}
\begin{enumerate}[noitemsep]
    \item \textbf{Glass room baseline noise exceeds 45~dBA detection threshold:} Likelihood: High. Site survey of Snell Library 4th floor glass room predicts baseline 42--48~dBA (glass wall STC 26--28 allows corridor noise transmission + small volume 9~m² + high occupancy density 5--6~students = cumulative noise sources). If baseline $\geq$45~dBA, system generates continuous ``violation'' status even during compliant behavior (no conversations). Mitigation: (a) Site survey already conducted (Fig.~\ref{fig:glass_room} shows glass room 
predicted 42--48~dBA baseline); 24-hour characterization will validate prediction, (b) if baseline $>$45~dBA sustained, implement zone-specific threshold adjustment (48--50~dBA for glass rooms vs 45~dBA for solid walls), (c) document as ``placement quality issue'' validating core problem statement (posted ``SILENT'' zone fails to meet acoustic standard - system correctly identifies this), (d) use data to recommend facility improvements (add acoustic panels \$500, limit occupancy to 3~people, or reclassify as ``Quiet Zone'' 50~dBA vs ``Silent Study'' 45~dBA). This is not system failure - it's system working correctly by detecting poorly-placed quiet zones.
    
    \item \textbf{Calibration accuracy fails to achieve $\pm$3~dBA target:} Likelihood: Medium. Consumer-grade reference meter (BAFX3608 $\pm$1.5~dBA) contributes 50\% of error budget; curve-fitting residuals may exceed $\pm$1~dBA assumption if microphone response non-linear. Mitigation: (a) Use ANSI S1.4 Class~2 certified reference meter (BAFX3608 specified, not uncertified smartphone apps), (b) conduct calibration in controlled quiet environment ($<$30~dBA ambient, minimal reflective surfaces to reduce multipath), (c) perform 6-point curve fit with validation at intermediate levels (45, 65, 85~dBA check linearity), (d) if RSS error $>$$\pm$3~dBA persists, consider INMP441 I2S digital microphone upgrade (\$7, 61~dBA SNR similar but digital path eliminates ADC quantization, potentially $\pm$2~dBA achievable), (e) fallback acceptable: relative measurements (``Zone A baseline 150~ADC units, Zone B baseline 600~units, therefore Zone B 4$\times$ louder'') still provide valuable placement quality comparison without absolute dBA calibration.
    
    \item \textbf{WiFi connectivity unreliable in test environment:} Likelihood: Low. However, enterprise WiFi (eduroam WPA2-Enterprise) may block IoT devices; residential WiFi may have insufficient coverage in testing location. Mitigation: (a) Pre-test WiFi coverage at intended sensor locations using smartphone WiFi analyzer app (verify RSSI $>$-70~dBAm, confirm access point visibility), (b) position sensors with line-of-sight to nearest WiFi router (avoid placement behind metal filing cabinets or inside metal enclosures which attenuate 2.4~GHz RF), (c) implement firmware retry logic (3~connection attempts with exponential backoff: 1s, 2s, 4s delays before declaring failure), (d) local buffering in ESP8266 4MB flash memory (circular buffer: 120~readings = 1~hour @ 30s interval, batch upload on reconnection with original timestamps), (e) watchdog timer resets MCU if WiFi down $>$5~minutes (prevents infinite hang state), (f) fallback: use mobile phone hotspot for lab testing if home WiFi inadequate (always works, controlled SSID/password); (g) coordinate with Northeastern IT for IoT device registration if eduroam blocks MAC address (submit registration request Week~1 parallel to component ordering).
\end{enumerate}

\vspace{0.5em}

\noindent\textbf{Key Assumptions:}
\begin{enumerate}[noitemsep]
    \item \textbf{WiFi infrastructure available and accessible in target facilities:} Consequence if false: Must add LoRa gateway (\$150 hardware) + cellular backhaul SIM card (\$10/month data plan) or Zigbee coordinator (\$50), increasing system cost by 50--100\% and eliminating primary cost advantage of WiFi reuse; implementation timeline extends by 2+ weeks for gateway configuration and custom network protocol. Confidence: High for modern institutional settings (libraries, hospitals, airports all provide WiFi for public/staff use); validated by Northeastern campus site survey (eduroam/NUwave enterprise WiFi detected with RSSI -45 to -55~dBAm excellent signal strength); healthcare facility likely has guest WiFi (industry standard for patient/family connectivity). Risk mitigation: Confirmed via physical site visit and smartphone WiFi scanning; worst-case fallback to mobile hotspot for proof-of-concept demonstration (validates technical functionality even if facility WiFi unavailable).
    
    \item \textbf{Quiet zone acoustic standard is 40--50~dBA SPL:} Consequence if false: If facilities require stricter standard ($<$35~dBA, e.g., recording studios, meditation centers), MAX9814 noise floor (33~dBA SPL equivalent from 61~dBA SNR) approaches measurement limit, reducing detection margin from 12~dBA to 2~dBA; accuracy may degrade to $\pm$5~dBA in 35--40~dBA range; would necessitate upgrade to studio-grade measurement microphone (\$40--100 with $<$25~dBA noise floor) or precision MEMS sensor. Confidence: Medium-High; ANSI S12.60-2010 specifies 35~dBA background maximum for core learning spaces (classrooms, libraries similar); WHO Guidelines for Community Noise (1999) recommend $<$35~dBA for sleep areas, $<$40~dBA for living areas; observed library signage posts ``SILENT STUDY'' without numeric threshold (industry practice: 40--50~dBA typical per Patel et al., 2015, Journal of Academic Librarianship); healthcare ``Reflection Room'' likely 40--45~dBA standard. Risk: Acceptable; if $<$35~dBA required, document sensor limitation and recommend precision alternative for production.
    
    \item \textbf{Bench/lab testing environment representative of actual facility deployment:} Consequence if false: Controlled laboratory testing (quiet room, no foot traffic, predictable HVAC) may not reveal issues specific to real institutional environments: (a) RF interference from elevator motors, fluorescent lighting, medical equipment (degrades WiFi), (b) acoustic reflections from specific room geometries not anticipated (corner resonances, flutter echoes from parallel walls affecting RMS calculations), (c) user behavioral factors (students moving sensors, unplugging for phone charging, deliberate threshold testing), (d) institutional IT policies (firewall blocking outbound HTTPS, MAC address filtering, bandwidth throttling). Confidence: Medium; bench testing validates technical functionality (sensor accuracy, WiFi transmission protocol, threshold detection logic, calibration procedure) but cannot simulate all human/environmental interactions of real deployment. Mitigation: Phased validation approach: (1) lab/dorm controlled testing establishes baseline performance, (2) limited 1-week pilot in single accessible campus location (e.g., study room in student center with permission), (3) comparison of lab vs field data identifies deployment-specific adjustments needed, (4) full facility deployment only after pilot success. Accept that some unknowns remain until field deployment; design for adaptability (configurable thresholds, firmware updates via USB).
\end{enumerate}

\vspace{0.5em}

\noindent\textbf{Unknowns to Resolve:}
\begin{enumerate}[noitemsep]
    \item \textbf{Optimal sustained violation duration threshold (currently 5~minutes):} Current design: Flag violation if dBA $\geq$45 for 5~consecutive minutes (300~seconds). Unknown: Is 5~minutes optimal balance between false alarm suppression (door slams $<$30~sec, brief conversations 1--2~min correctly filtered) vs detection sensitivity (sustained conversations $>$5~min flagged)? Alternative durations: 1~min (more sensitive, higher false alarm risk from longer conversations near threshold), 3~min (moderate), 10~min (very conservative, may miss shorter violations). Resolution plan: Empirical testing during Week~5-6 validation period; simulate controlled scenarios (play conversation recording for 2, 4, 6, 8, 10~minute durations at 60~dBA; measure false positive rate for door slam events; collect user feedack on acceptable detection latency); analyze receiver operating characteristic (ROC) curve (false positive rate vs true positive rate for durations 1--10~min); select duration maximizing detection while keeping false alarms $<$5\% of events; document rationale and ROC analysis in final technical report.
    
    \item \textbf{Measurement accuracy at low end of range (35--40~dBA near noise floor):} MAX9814 noise floor 33~dBA SPL provides only 2--7~dBA margin for 35--40~dBA measurements (vs 12~dBA margin at 45~dBA). Unknown: Does $\pm$3~dBA accuracy hold at bottom of range, or does sensor self-noise dominate causing accuracy degradation to $\pm$5--10~dBA for very quiet conditions? This affects baseline characterization accuracy (determining ``how quiet'' a well-placed zone actually is). Resolution plan: During Week~4 calibration, test specifically at 35, 37, 40~dBA reference levels in addition to standard 40, 50, 60, 70, 80, 94~dBA points; measure repeatability (10~trials per level, calculate standard deviation); if standard deviation $>$2~dBA at $<$40~dBA levels, document limited low-end accuracy and recommend: (a) INMP441 I2S MEMS upgrade (\$7, 61~dBA SNR but digital path, lower noise floor 29~dBA SPL) for applications requiring precision $<$40~dBA measurements, or (b) accept reduced accuracy at low end ($\pm$5~dBA for 35--40~dBA range) since primary detection occurs at 45--70~dBA where accuracy excellent. Not critical for MVP (violations occur 60--70~dBA, well above noise floor) but important for complete characterization.
\end{enumerate}

\clearpage

% ============================================================================
% SECTION 8: MVP SCOPE AND COURSE FEASIBILITY
% ============================================================================
\section{MVP Scope and Course Feasibility}
\label{sec:mvp}

\noindent\textbf{MVP Definition:}\\
\textit{In Scope:} (1) Build 2~sensor nodes: Snell Library open study area (glass construction, baseline 40--45~dBA predicted) + Healthcare facility Reflection Room (solid drywall, baseline 35--40~dBA predicted) for construction type comparison validating placement quality detection, (2) hardware assembly: ESP8266 NodeMCU v3 + Adafruit MAX9814 breakout + USB power + enclosures + wall mounting, (3) firmware development: ESP8266 Arduino core, analogRead() ADC sampling at 1~kHz, 1-second RMS calculation over 1000-sample window, linear calibration curve application (m $\times$ ADC + b coefficients from EEPROM), 10-sample moving average filter, sustained violation detection logic (5-minute timer), WiFi connection to campus network or mobile hotspot, HTTP POST to ThingSpeak API every 30~seconds with JSON payload, NTP time synchronization, serial console debug output, (4) cloud infrastructure: 2~ThingSpeak channels (free tier, 1~per sensor), data retention 2+ weeks, (5) web dashboard: HTML/CSS/JavaScript client polling ThingSpeak API every 30~seconds, color-coded status display (green $<$45~dBA compliant, red $\geq$45~dBA violation), last update timestamp, current dBA level numeric display, 2-zone comparison view, (6) calibration validation: 6-point procedure (40, 50, 60, 70, 80, 94~dBA) using BAFX3608 reference meter, linear regression curve fit, $\pm$3~dBA accuracy verification at all points, intermediate validation (45, 65, 85~dBA), (7) 48-hour minimum continuous operation data collection (system uptime $>$90\%, packet delivery $>$99\%), (8) controlled testing scenarios: baseline characterization (empty room overnight 3--6~AM measures true environmental floor), compliant state (40~dBA sustained 10~minutes, verify green status), violation detection (60~dBA conversation recording 10~minutes, verify red status after 5-min delay), transient filtering (5$\times$ door slam events $<$10~sec each, verify no false violation alerts), placement quality validation (compare 2-zone baselines, demonstrate 10--15~dBA difference detection), (9) technical report: PES documentation + validation results + measured data plots + trade-off analysis + lessons learned.\\
\textit{Out of Scope:} Frequency-domain analysis (FFT, spectrograms, speech recognition - would require 8+ kHz Nyquist sampling), machine learning predictions (occupancy estimation, acoustic event classification), mobile applications (iOS/Android native apps; web dashboard sufficient for MVP), actual Snell Library deployment with institutional permissions (if not obtained by Week~6; lab testing proves concept adequately), SMS/push notifications (email alerts acceptable for facility staff; real-time dashboard for users), gateway architecture (unnecessary for 2--3 sensors with direct WiFi), multi-facility scaling beyond proof-of-concept (commercial deployment deferred), advanced analytics (long-term trending, pattern detection, predictive maintenance).

\vspace{0.5em}

\noindent\textbf{Success Criteria:}\\
Technical validation: (1) Measurement accuracy $\pm$3~dBA verified at 6~calibration points (40--95~dBA range) against BAFX3608 reference meter, all points within tolerance, (2) end-to-end latency $<$120~seconds measured via controlled acoustic event (hand clap at known time) to dashboard display update timestamp difference (95\% of events meet requirement), (3) system uptime $>$90\% over 48-hour continuous test period (maximum 4.8~hours cumulative downtime acceptable via heartbeat gap analysis), (4) packet delivery rate $>$99\% over 24-hour period (transmitted packet count via serial console vs received packet count via ThingSpeak API query, sequence number gap detection), (5) state discrimination functional (correctly classifies $<$45~dBA as compliant and $\geq$60~dBA as violation in 10-minute controlled tests, 0\% false positives and 0\% false negatives required). Functional validation: (6) Multi-zone dashboard operational displaying 2~sensors concurrently with independent status updates, (7) transient filtering effective (5$\times$ door slam tests at 60~dBA peak, $<$10~second duration each produce 0~violation alerts due to 5-minute sustained logic), (8) placement quality detection demonstrated (system measures and displays 10--15~dBA baseline difference between solid-wall room 35--40~dBA vs glass-wall room 42--48~dBA, validating core problem statement that placement affects achievable quiet levels). Success defined as all 8~criteria met; partial success (6--7 criteria) acceptable with documented limitations and mitigation plans for production system.

\vspace{0.5em}

\noindent\textbf{Resource Requirements:}\\
\textbf{Hardware:} \$93 sensor nodes (2$\times$ @ \$31 minimum, 3$\times$ @ \$93 if stretch goal) + \$55 validation equipment (sound meter \$30, multimeter \$15, breadBAoard \$10) = \$148 total within \$200 budget constraint. Components available with 3--7~day shipping (Amazon Prime, Adafruit standard delivery). \textbf{Software:} All free/open-source tools: Arduino IDE (ESP8266 core libraries v3.1+), ThingSpeak free tier (MathWorks, no credit card required for $<$3M msg/year), web hosting via GitHub Pages (free static site) or local HTML file (no server needed). \textbf{Lab access:} Home/apartment or university makerspace for bench assembly and testing (no special facilities required; standard desk workspace with WiFi and power outlet sufficient). \textbf{Network access:} Campus WiFi (eduroam/NUwave) or mobile phone hotspot (fallback, always available). \textbf{Facility access:} Snell Library publicly accessible for site survey photos (no permission needed); actual sensor deployment requires facilities approval (submitted request Week~1, approval timeline 2--4~weeks uncertain; not critical path blocker since lab testing validates technical function). \textbf{Time availability:} Estimated 40--50~hours total effort over 10~weeks (4--5~hours/week average): hardware assembly 6~hours, firmware development 12~hours, calibration 4~hours, testing/validation 10~hours, data analysis 6~hours, report writing 12~hours. Fits within typical course project workload expectations.

\vspace{0.5em}

\noindent\textbf{Timeline Assessment:}\\
Remaining time: 10~weeks (February~4 - April~15 final demonstration). Critical path: Weeks~1-4 (component ordering + firmware development, no schedule slack). Buffer: 2~weeks (Weeks~9-10 for unexpected debugging or documentation time overruns). Confidence level: \textbf{High} that MVP is achievable within timeline. Detailed breakdown: \textbf{Week~1 (Feb~4-10):} Order all components immediately upon PES approval (Amazon/Adafruit, 3--7~day shipping), start firmware development in parallel using ESP8266 simulator or dummy data (WiFi connection code, ThingSpeak API POST function testable without hardware). \textbf{Week~2 (Feb~11-17):} Component delivery expected; inventory parts, assemble first sensor node on breadBAoard, test power supply (verify 3.3V regulation, measure current draw), test WiFi connection (associate to campus network or hotspot, verify internet reachability, test ThingSpeak POST with static dummy JSON), validate ADC reading (connect MAX9814, observe voltage changes with voice/noise, serial console printf debugging). \textbf{Week~3 (Feb~18-24):} Complete firmware implementation (RMS calculation over 1000-sample buffer, calibration curve placeholder with default coefficients, moving average filter, threshold detection state machine), assemble second sensor node, test multi-node simultaneous operation. \textbf{Week~4 (Feb~25-Mar~3):} Calibration procedure execution (obtain BAFX3608 meter, generate test tones 40--94~dBA using smartphone app + Bluetooth speaker in quiet room, record ADC vs reference dBA pairs, perform linear regression in spreadsheet, program coefficients to EEPROM), validate calibration at intermediate levels, iterate if accuracy $>$$\pm$3~dBA. \textbf{Weeks~5-6 (Mar~4-17):} System validation testing (48--96~hour continuous operation for uptime measurement, controlled scenario tests for all success criteria VAL-1 through VAL-8, data collection and logging, identify any firmware bugs or hardware issues). \textbf{Weeks~7-8 (Mar~18-31):} Data analysis (plot time-series dBA levels, calculate baseline statistics, measure false alarm rate, compare 2-zone performance), dashboard refinement (improve UI based on initial testing feedBAack), troubleshooting any reliability issues discovered during continuous operation. \textbf{Week~9 (Apr~1-7):} Technical report writing (document PES updates, validation results with plots/tables, trade-off analysis, lessons learned), prepare demonstration materials. \textbf{Week~10 (Apr~8-15):} Final presentation preparation, rehearsal, buffer for last-minute issues; final demo April~15. \textbf{Risk assessment:} Primary risk is Week~1-2 component delivery delay (shipping disruptions, stock shortages); mitigation via immediate ordering and parallel firmware development allows productive work even if hardware delayed. Secondary risk is Week~4 calibration difficulty; mitigation via fallback to relative measurements if absolute calibration fails. Overall timeline realistic with comfortable margins; modular design (sensors work independently) allows incremental validation reducing integration risk.

\clearpage

% ============================================================================
% SECTION 9: CONSISTENCY CHECKS
% ============================================================================
\section{Consistency Checks}
\label{sec:consistency}

\begin{enumerate}
    \item \textbf{Are all units consistent throughout the document?}\\
    Answer: Yes\\
    Justification: All data rates in bps (Section~\ref{sec:traffic}: 307~bps per-node, 921~bps aggregate, 1,382~bps peak), all acoustic levels in dBA SPL (Section~\ref{sec:signals}: 35--100~dBA range, 45~dBA threshold, Section~\ref{sec:problem}: 50--65~dBA poor placement), all distances in meters (Section~\ref{sec:deployment}: 800~m² area, 4$\times$5~m typical room), all costs in USD (Section~\ref{sec:bom}: \$148 total, \$31 per sensor), all times in seconds (Section~\ref{sec:signals}: 30~s reporting, Section~\ref{sec:traffic}: 30~s interval, 1~s sampling). Verified consistency by searching document for each unit type.
    
    \item \textbf{Does the sampling rate in Section~\ref{sec:signals} match the traffic calculations in Section~\ref{sec:traffic}?}\\
    Answer: Yes\\
    Justification: Section~\ref{sec:signals} specifies 1~Hz effective sampling rate (1-second RMS window producing one dBA value per second) with transmission every 30~seconds. Section~\ref{sec:traffic} uses 30-second reporting interval in all calculations (1,151~bytes / 30~s = 307~bps). Internal sampling (1~kHz ADC $\rightarrow$ 1~Hz RMS) vs external reporting (30~sec transmission) correctly distinguished. Table~\ref{tab:traffic} ``Sampling Interval'' row states 1~s (internal processing rate), ``Reporting Interval'' row states 30~s (network transmission rate). Values consistent and properly documented.
    
    \item \textbf{Does the node count in Table~\ref{tab:deployment} match the traffic calculation in Table~\ref{tab:traffic}?}\\
    Answer: Yes\\
    Justification: Table~\ref{tab:deployment} specifies ``2--3'' sensor nodes (MVP minimum 2, stretch goal 3). Table~\ref{tab:traffic} calculations explicitly use 3~nodes (worst-case for capacity analysis: 3 $\times$ 307~bps = 921~bps aggregate). Using maximum node count (3) in traffic calculations provides conservative estimate; actual deployment with 2~nodes would have lower traffic (614~bps) with even more headroom. Approach consistent: always calculate worst-case for capacity planning.
    
    \item \textbf{Are the gateway/backhaul assumptions consistent with the aggregate traffic rate?}\\
    Answer: Yes\\
    Justification: Table~\ref{tab:deployment} specifies 0~gateways (direct WiFi to cloud architecture). Section~\ref{sec:deployment} topology: ``star with sensors connecting directly to existing facility WiFi.'' Aggregate traffic rate 921~bps (Table~\ref{tab:traffic}) represents 0.013\% of WiFi 11~Mbps minimum capacity (802.11g). No gateway needed for this traffic level (8,000$\times$ below capacity). If traffic were higher (e.g., 100+ sensors = 30~kbps), gateway for local aggregation might be justified, but not for 2--3~node proof-of-concept. Gateway count zero consistent with star topology and traffic demand.
    
    \item \textbf{Are the prioritized constraints from Section~\ref{sec:constraints} reflected in design choices throughout?}\\
    Answer: Yes\\
    Justification: Constraint~\#1 (cost $<$\$200) drives: ESP8266 selection (\$6 vs RPi \$35, Section~\ref{sec:signals}), WiFi reuse (0~gateways, Section~\ref{sec:deployment}, saves \$150), 2--3~sensor limit (Table~\ref{tab:deployment}), analog microphone choice (MAX9814 \$10 with 2-hour implementation prioritized over I2S alternatives despite \$3 higher cost, justified by schedule risk reduction). Constraint~\#2 ($\pm$3~dBA accuracy) drives: 6-point calibration procedure (Section~\ref{sec:bom} maintenance model), MAX9814 SNR requirement (61~dBA provides 12~dBA margin, Section~\ref{sec:signals}), error budget analysis showing $\pm$1.6~dBA RSS prediction. Constraint~\#3 (10-week timeline) drives: analog vs I2S trade-off (Section~\ref{sec:constraints} conflict resolution), existing WiFi vs custom network (Section~\ref{sec:deployment}), phased testing approach (Section~\ref{sec:mvp} lab $\rightarrow$ facility). All major design decisions traceable to top~3 constraints. Lower-priority constraints (latency~\#4, power~\#5) inform but don't drive architecture (power unconstrained enables simplifications, latency sets 30-sec interval but not critical path).
    
    \item \textbf{Is the duty cycle in Table~\ref{tab:deployment} consistent with latency requirements?}\\
    Answer: Yes\\
    Justification: Table~\ref{tab:deployment} specifies 100\% duty cycle (continuous monitoring, wall-powered, always-on WiFi). Section~\ref{sec:constraints} Constraint~\#4 specifies $<$120~second latency requirement for user wayfinding. Continuous operation (no sleep cycles) enables 30-second reporting interval (Section~\ref{sec:traffic}) achieving worst-case latency 41--120~seconds (30~sec reporting + 11~sec processing: 1~sec RMS + 10~sec moving average). If duty-cycled (e.g., wake every 60~seconds for 1-second measurement to save battery), latency would increase to 60+ seconds plus wake-up time, potentially violating $<$120~sec requirement. Wall power availability (Section~\ref{sec:bom}: 445~mW, \$0.59/year) justifies 100\% duty cycle economically. Duty cycle supports latency constraint; power budget enables duty cycle choice. Internally consistent.
    
    \item \textbf{Does the BOM in Section~\ref{sec:bom} account for all node types mentioned?}\\
    Answer: Yes\\
    Justification: System architecture uses single node type: sensor nodes (ESP8266 + MAX9814 + power + enclosure). No gateway nodes (Table~\ref{tab:deployment} specifies 0~gateways, Section~\ref{sec:deployment} confirms direct WiFi to cloud). No relay nodes (star topology, not mesh). Section~\ref{sec:bom} BOM lists: ``Sensor Nodes: 3~units @ \$31 each = \$93'' with detailed per-node breakdown (ESP8266 \$6, MAX9814 \$10, USB cable \$3, adapter \$2, enclosure \$8, mounting \$2 totaling \$31 verified). Gateway line item: ``0~units @ \$0'' explicitly stated. All node types (only sensor nodes exist) fully accounted for in BOM. Total \$148 includes validation equipment (\$55) beyond node hardware. No missing node types; BOM complete and consistent with architecture.
\end{enumerate}

\clearpage

% ============================================================================
% APPENDIX: AI USE AND ATTESTATION
% ============================================================================
\appendix
\section{AI Use and Attribution}
\label{sec:ai}

\noindent\textbf{AI Tools Used:}\\
Claude AI (Anthropic) - Conversational AI assistant accessed via claude.ai web interface, Sonnet 4.5 model.

\vspace{0.5em}

\noindent\textbf{How AI Was Used:}\\
(1) \textbf{Problem scope refinement:} Discussed various IoT application areas (school noise monitoring, environmental sensing, quiet zone verification) through multiple iterations to identify focused, achievable scope within student project constraints (budget \$200, timeline 10~weeks, solo implementation). AI helped narrow from overly broad ``campus noise monitoring'' to specific ``designated quiet zone verification in institutional facilities'' with concrete problem statement (poor placement, no wayfinding, no compliance data).
(2) \textbf{Technical framework guidance:} Consulted on standard IoT system design methodology: defining problem $\rightarrow$ constraints $\rightarrow$ feasibility checks $\rightarrow$ component selection $\rightarrow$ traffic analysis. AI provided structure for constraint prioritization (cost $>$ accuracy $>$ timeline), signal chain architecture (acoustic $\rightarrow$ electrical $\rightarrow$ digital $\rightarrow$ network $\rightarrow$ cloud $\rightarrow$ display), error budget analysis (RSS combination of independent error sources). 
(3) \textbf{Document organization:} AI helped map my technical content into required PES template sections (8~sections + consistency checks + AI disclosure), suggested LaTeX table formatting for deployment parameters and traffic budget, provided examples of how to present trade-off matrices (MAX9814 vs SPH0645 decision table format). 
(4) \textbf{Trade-off analysis structuring:} Discussed microphone selection rationale; AI helped articulate decision framework (implementation time 2hr vs 8hr more important than SNR 61~dBA vs 65~dBA given both meet $\pm$3~dBA requirement), suggested quantitative comparison table format showing factors/winners. 
(5) \textbf{Calculations verification:} Used AI as calculator check: traffic budget arithmetic (1,151~bytes / 30s $\times$ 8 = 307~bps verified), power consumption (81~mA $\times$ 3.3V = 267~mW verified), error budget RSS ($\sqrt{1^2 + 0.5^2 + 1^2 + 0.5^2}$ = 1.6~dBA verified), protocol overhead (94~byte payload $\rightarrow$ 1,151~byte total verified by layer-by-layer addition). 
(6) \textbf{Sampling rate justification:} Discussed Nyquist theorem applicability; AI explained when sub-Nyquist sampling acceptable (amplitude-only measurement via RMS, aliasing preserves total energy per Parseval's theorem, frequency content irrelevant for dBA SPL calculation), helped structure technical explanation for 1~kHz sampling of 4~kHz signal bandwidth. (7) \textbf{Writing assistance:} AI helped convert technical bullet points into flowing paragraph text meeting word count targets (150--250~words per section), suggested transitions between sections, identified redundant content for removal.

\vspace{0.5em}

\noindent\textbf{What I Verified:}\\
\textbf{All component specifications:} Cross-referenced with actual manufacturer datasheets downloaded and reviewed: (a) MAX9814 Datasheet (Maxim Integrated, Rev~2) - extracted SNR 61~dBA (Table~2, pg.~4), power consumption 3.1~mA (Table~2), gain settings 40/50/60~dBA (Table~2), frequency response 100~Hz - 10~kHz (Gain vs Frequency graph, Typical Operating Characteristics section), AGC timing attack 1.1~ms / release 4400~ms (Applications Information, Table~2), temperature range -40 to +85\textdegree{}C (Ordering Information), output voltage 3~mV to 2.45V (Table~1), MICBIAS 2.0V (graph); sensitivity -44~dBAV/Pa from Adafruit \#1713 documentation (Maxim datasheet page not located but value industry-standard for electret mics, cross-checked with similar Knowles SPM0408 spec -42~dBAV/Pa for validation), (b) ESP8266EX Datasheet v6.7 (Espressif Systems, 2023) - WiFi power consumption 140~mA TX / 56~mA RX (Table~5-3, pg.~18), receiver sensitivity -91~dBAm @ 11~Mbps (Section~3, Electrical Specifications), operating voltage 2.7--3.6V, CPU 80~MHz, ADC 10-bit (Section~3.6.2), RAM 160~KB total. \textbf{All calculations:} Manually verified using calculator and Excel spreadsheet: (a) Traffic: 1,151~bytes / 30~s = 38.4~bytes/s; 38.4 $\times$ 8 = 307~bps; 307 $\times$ 3~nodes = 921~bps; 921 / 11,000,000~WiFi capacity = 0.0084\% utilization $\checkmark$, (b) Power: ESP8266 (140$\times$0.5 + 80$\times$29.5)/30 = 81~mA; MAX9814 3.1~mA; regulator 5~mA; total 89.1~mA; 89.1 $\times$ 5V = 445~mW; 445~mW $\times$ 8760~hr = 3.9~kWh/year $\checkmark$, (c) Error budget RSS: $\sqrt{1^2 + 0.5^2 + 1^2 + 0.5^2} = \sqrt{2.5}$ = 1.58 $\approx$ 1.6~dBA $\checkmark$, (d) Noise floor: 94~dBA SPL - 61~dBA SNR = 33~dBA noise floor; 45~dBA threshold - 33~dBA = 12~dBA margin $\checkmark$, (e) Coverage: 60~dBA @ 1m - 20$\times$log$_{10}$(5) = 60 - 14 = 46~dBA @ 5m; 46 - 45 = 1~dBA margin $\checkmark$, (f) Cost: (3 $\times$ \$31~sensors) + \$55~validation = \$148~total $\checkmark$. \textbf{Acoustic theory:} Validated against textbook references: Sound pressure level definition SPL = 20log$_{10}$(P/P$_{ref}$) where P$_{ref}$ = 20~µPa (ISO 1996-1:2016, Acoustics - Description and measurement of environmental noise), spherical spreading loss = 20log$_{10}$(r) for free-field propagation (Harris, HandBAook of Acoustical Measurements, 2006, Chapter~12), Nyquist-Shannon sampling theorem f$_s$ $\geq$ 2f$_{max}$ (Oppenheim \& Schafer, Discrete-Time Signal Processing, 2009, Chapter~4), Parseval's theorem energy conservation under aliasing (same reference). \textbf{Standards:} Reviewed primary sources for quiet zone thresholds: WHO Guidelines for Community Noise (1999, Table~4.1: $<$35~dBA for sleep, $<$40~dBA living areas), ANSI S12.60-2010 (classrooms 35~dBA background max), cross-referenced with library industry practices (Patel et al., 2015, J. Academic Librarianship: 40--50~dBA typical posted standard). \textbf{Site survey:} Personally conducted January~28, 2026: visited Northeastern 
Snell Library (Floor 2 open study, 4th floor glass rooms - Figs.~\ref{fig:glass_room}, 
\ref{fig:yellow_room}), local elderly care center (Reflection Room - Fig.~\ref{fig:reflection}), 
photographed signage (Fig.~\ref{fig:sign}) and construction types (4 photos obtained, 
included in document), observed glass vs drywall materials, estimated room dimensions 
via paced steps (4--5~m typical), verified WiFi availability via smartphone WiFi 
analyzer (eduroam/NUwave detected, RSSI -45 to -55~dBm measured), noted HVAC noise 
sources and corridor traffic patterns. All site observations firsthand; photos taken 
with personal smartphone camera.

\vspace{0.5em}

\noindent\textbf{Engineering Judgment:}\\
All trade-offs (MAX9814 vs SPH0645, ESP8266 vs ESP32, analog vs I2S, star vs mesh, 30~sec vs 10~sec reporting) represent my engineering judgment based on constraint priorities I established (cost~\#1, accuracy~\#2, timeline~\#3); AI helped structure presentation but did not make decisions. I can defend every choice with quantitative reasoning (schedule risk 6~hours valued over SNR 4~dBA improvement; \$27 cost savings from 10-bit ADC justified by 50$\times$ resolution margin vs accuracy requirement; 30-sec interval chosen via latency requirement $<$120s and API limit 3M~msg/year calculation). I understand complete system end-to-end: physical acoustics (sound pressure wave propagation, STC transmission loss through glass), sensor operation (electret capacitance transduction, AGC behavior), signal processing (RMS amplitude calculation, moving average FIR filter), network protocols (HTTP POST structure, TCP 3-way handshake, WiFi CSMA/CA), and can explain during oral examination without notes or AI assistance.

\vspace{0.5em}

\noindent\textbf{Attestation:}\\
I confirm that I understand all content in this document and can explain and defend every claim, calculation, and design decision during oral examination. I have personally reviewed all cited datasheets (MAX9814 Rev~2, ESP8266EX v6.7), performed all calculations independently with verification, conducted the site survey with firsthand observations, and made all engineering trade-off decisions based on my analysis of constraints and requirements. I can derive the traffic budget from packet structure through protocol layers, explain the microphone selection trade-off with quantitative comparison of implementation time vs SNR benefit, justify 1~kHz sampling rate using Nyquist theorem and amplitude measurement theory, defend $\pm$3~dBA accuracy target via error budget and state discrimination analysis (45~dBA vs 60~dBA separation), and describe why star topology with WiFi is appropriate for this deployment scale and traffic pattern. I take full responsibility for technical correctness of all specifications and calculations.

\vspace{1em}

\noindent Signature: \underline{\hspace{5cm}} \hfill Date: \underline{February~4, 2026\hspace{1cm}}

% ============================================================================
% END OF DOCUMENT
% ============================================================================
\end{document}


